# -*- coding: utf-8 -*-
"""CSC490-YOLO-FinalProject.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FiPwm7lmfr8RomWLkcwlx90TKFyduEDH

# ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏•‡∏∞ Import Library
"""

!pip install pythainlp -q
!pip install lime -q
!pip install imbalanced-learn -q

import os
import re
import glob
import csv
import string
import joblib
import warnings
from datetime import datetime

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from pythainlp.tokenize import word_tokenize
from pythainlp.corpus import thai_stopwords

from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.preprocessing import LabelEncoder
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.pipeline import Pipeline
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

from sklearn.naive_bayes import MultinomialNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier

import lime
import lime.lime_text

from google.colab import drive

from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler
from imblearn.over_sampling import SMOTE
from imblearn.pipeline import Pipeline as ImbPipeline
import joblib
import os

"""# ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ Drive & ‡πÇ‡∏´‡∏•‡∏î Repository
Repository ‡∏à‡∏≤‡∏Å https://github.com/ssivakorn/Thai-SMS-spam-filter.git
"""

try:
    drive.mount('/content/drive')

    # ‡∏ó‡∏µ‡πà‡∏à‡∏±‡∏î‡πÄ‡∏Å‡πá‡∏ö‡∏´‡∏•‡∏±‡∏Å‡∏Ç‡∏≠‡∏á‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Å‡∏ï‡πå
    DRIVE_PROJECT_PATH = '/content/drive/MyDrive/SMS_Scam_Final_Project/'
    os.makedirs(DRIVE_PROJECT_PATH, exist_ok=True)

    print(f"‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à - ‡πÑ‡∏ü‡∏•‡πå‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡∏à‡∏±‡∏î‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏ß‡πâ‡∏ó‡∏µ‡πà : {DRIVE_PROJECT_PATH}")

except Exception as e:
    print(f"\n‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ : {e}")

print("‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•")

repo_name = 'Thai-SMS-spam-filter'
repo_path = os.path.join(DRIVE_PROJECT_PATH, repo_name)
repo_url = 'https://github.com/ssivakorn/Thai-SMS-spam-filter.git'

if not os.path.exists(repo_path):
    print(f"‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î {repo_path}")
    !git clone {repo_url} {repo_path} -q
    print("‡πÇ‡∏´‡∏•‡∏î Repository ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")
else:
    print(f"‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß")

# ‡∏≠‡πà‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡∏ó‡∏∏‡∏Å‡πÑ‡∏ü‡∏•‡πå
data_path = os.path.join(repo_path, 'data/raw_data/')
all_files = glob.glob(os.path.join(data_path, '*.txt'))
all_data = []
print(f"\n‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î {len(all_files)} ‡πÑ‡∏ü‡∏•‡πå ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î")

for file_path in all_files:
    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            parts = line.strip().split('\t', 1)
            if len(parts) < 2: parts = line.strip().split('|', 1)
            if len(parts) == 2:
                all_data.append({'label': parts[0], 'message': parts[1]})

# ‡∏™‡∏£‡πâ‡∏≤‡∏á DataFrame
if all_data:
    print(all_data)
    df = pd.DataFrame(all_data)
    df = df.sample(frac=1, random_state=42).reset_index(drop=True)
    print("\n‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏•‡∏∞‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")
    print(f"‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î {len(df)} ‡πÅ‡∏ñ‡∏ß")
    display(df['label'].value_counts())
else:
    print("\n‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏î‡πâ")

"""# ‡∏ï‡∏±‡∏î Emoji & ‡∏™‡∏£‡πâ‡∏≤‡∏á Features & ‡∏Å‡∏≥‡∏´‡∏ô‡∏î Label ‡πÅ‡∏•‡∏∞‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å label_encoder"""

def text_process(text: str) -> str:
    if not isinstance(text, str):
        return ""

    # ‡∏ï‡∏±‡∏î Emoji
    emoji_pattern = re.compile(
        "["
        u"\\U0001F600-\\U0001F64F"
        u"\\U0001F300-\\U0001F5FF"
        u"\\U0001F680-\\U0001F6FF"
        u"\\U0001F1E0-\\U0001F1FF"
        u"\\U00002702-\\U000027B0"
        u"\\U000024C2-\\U0001F251"
        "]+", flags=re.UNICODE)
    text = emoji_pattern.sub(r'', text) # ‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà emoji ‡∏î‡πâ‡∏ß‡∏¢‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ß‡πà‡∏≤‡∏á

    tokens = word_tokenize(text, engine='newmm')
    stopwords_list = list(thai_stopwords())
    punctuations_to_remove = string.punctuation.replace('%', '')

    processed_tokens = [
        word for word in tokens
        if word not in stopwords_list and word not in punctuations_to_remove
    ]

    return " ".join(processed_tokens)


def create_features(df_input: pd.DataFrame) -> pd.DataFrame:
    df_out = df_input.copy()  # ‡∏ó‡∏≥‡∏™‡∏≥‡πÄ‡∏ô‡∏≤‡∏Ç‡∏≠‡∏á DataFrame

    # Statistical Features
    df_out['message_length'] = df_out['message'].str.len()
    # ‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÉ‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
    # Spam/Scam ‡∏°‡∏±‡∏Å‡∏à‡∏∞‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡∏ú‡∏¥‡∏î‡∏õ‡∏Å‡∏ï‡∏¥ ‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡∏™‡∏±‡πâ‡∏ô‡∏´‡∏£‡∏∑‡∏≠‡∏¢‡∏≤‡∏ß‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏£‡∏∞‡∏ï‡∏∏‡πâ‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏ô‡πÉ‡∏à

    df_out['digit_count'] = df_out['message'].str.count(r'\\d')
    # ‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÉ‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
    # Spam/Scam ‡∏°‡∏±‡∏Å‡∏à‡∏∞‡∏°‡∏µ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡πÄ‡∏¢‡∏≠‡∏∞ ‡πÄ‡∏ä‡πà‡∏ô ‡πÄ‡∏ö‡∏≠‡∏£‡πå‡πÇ‡∏ó‡∏£‡∏®‡∏±‡∏û‡∏ó‡πå, ‡∏£‡∏≤‡∏Ñ‡∏≤, ‡∏£‡∏´‡∏±‡∏™‡πÇ‡∏õ‡∏£‡πÇ‡∏°‡∏ä‡∏±‡πà‡∏ô, ‡∏´‡∏£‡∏∑‡∏≠‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏á‡∏¥‡∏ô‡∏Å‡∏π‡πâ

    df_out['uppercase_ratio'] = df_out['message'].str.count(r'[A-Z]') / df_out['message_length']
    # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏™‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©‡∏û‡∏¥‡∏°‡∏û‡πå‡πÉ‡∏´‡∏ç‡πà
    # Spam/Scam ‡∏°‡∏±‡∏Å‡πÉ‡∏ä‡πâ‡∏ï‡∏±‡∏ß‡∏û‡∏¥‡∏°‡∏û‡πå‡πÉ‡∏´‡∏ç‡πà‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏°‡∏≤‡∏Å (‡πÄ‡∏ä‡πà‡∏ô FREE!, ‡∏î‡πà‡∏ß‡∏ô!) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏∂‡∏á‡∏î‡∏π‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏ô‡πÉ‡∏à‡πÅ‡∏•‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏™‡∏∂‡∏Å‡πÄ‡∏£‡πà‡∏á‡∏î‡πà‡∏ß‡∏ô

    df_out['repeated_punctuation_count'] = df_out['message'].str.count(r'([!?.])\\1{1,}')
    # ‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏≠‡∏±‡∏Å‡∏Ç‡∏£‡∏∞‡∏û‡∏¥‡πÄ‡∏®‡∏©‡∏ã‡πâ‡∏≥‡πÜ (‡πÄ‡∏ä‡πà‡∏ô !!, ???)
    # ‡πÄ‡∏õ‡πá‡∏ô‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞‡∏ó‡∏µ‡πà‡∏û‡∏ö‡πÑ‡∏î‡πâ‡∏ö‡πà‡∏≠‡∏¢‡πÉ‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° Spam/Scam ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏ô‡πâ‡∏ô‡∏¢‡πâ‡∏≥‡πÅ‡∏•‡∏∞‡∏Å‡∏£‡∏∞‡∏ï‡∏∏‡πâ‡∏ô‡∏≠‡∏≤‡∏£‡∏°‡∏ì‡πå ‡πÅ‡∏ï‡πà‡πÑ‡∏°‡πà‡∏Ñ‡πà‡∏≠‡∏¢‡∏û‡∏ö‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏∑‡πà‡∏≠‡∏™‡∏≤‡∏£‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ

    # Pattern-based Features
    df_out['url_count'] = df_out['message'].str.count(r'https?://\\S+|www\\.\\S+|\\S+\\.\\S+')
    # ‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏•‡∏¥‡∏á‡∏Å‡πå URL ‡πÉ‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
    # ‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡πà‡∏≠‡∏á‡∏ó‡∏≤‡∏á‡∏´‡∏•‡∏±‡∏Å‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏´‡∏•‡∏≠‡∏Å‡πÉ‡∏´‡πâ‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏Ñ‡∏•‡∏¥‡∏Å‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡∏¢‡∏±‡∏á‡πÄ‡∏ß‡πá‡∏ö‡πÑ‡∏ã‡∏ï‡πå‡∏õ‡∏•‡∏≠‡∏°‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏ß‡πá‡∏ö‡πÇ‡∏Ü‡∏©‡∏ì‡∏≤

    short_url_pattern = 'bit.ly|cutt.ly|t.co|rebrand.ly|shorturl.asia|line.me|lin.ee'
    df_out['has_short_url'] = df_out['message'].str.contains(short_url_pattern, case=False, regex=True).astype(int)
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏•‡∏¥‡∏á‡∏Å‡πå‡∏¢‡πà‡∏≠‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
    # ‡∏°‡∏¥‡∏à‡∏â‡∏≤‡∏ä‡∏µ‡∏û‡∏ô‡∏¥‡∏¢‡∏°‡πÉ‡∏ä‡πâ‡∏•‡∏¥‡∏á‡∏Å‡πå‡∏¢‡πà‡∏≠‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏Å‡∏õ‡∏¥‡∏î URL ‡∏õ‡∏•‡∏≤‡∏¢‡∏ó‡∏≤‡∏á‡∏ó‡∏µ‡πà‡πÅ‡∏ó‡πâ‡∏à‡∏£‡∏¥‡∏á ‡∏ã‡∏∂‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏û‡∏§‡∏ï‡∏¥‡∏Å‡∏£‡∏£‡∏°‡∏ó‡∏µ‡πà‡∏ô‡πà‡∏≤‡∏™‡∏á‡∏™‡∏±‡∏¢

    phone_pattern = r'(\\d{2,3}-\\d{3,4}-\\d{4}|\\d{9,10})'
    df_out['has_phone'] = df_out['message'].str.contains(phone_pattern, regex=True).astype(int)
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ‡πÄ‡∏ö‡∏≠‡∏£‡πå‡πÇ‡∏ó‡∏£‡∏®‡∏±‡∏û‡∏ó‡πå‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
    # Spam/Scam ‡∏°‡∏±‡∏Å‡∏à‡∏∞‡πÉ‡∏™‡πà‡πÄ‡∏ö‡∏≠‡∏£‡πå‡πÇ‡∏ó‡∏£‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏£‡∏∞‡∏ï‡∏∏‡πâ‡∏ô‡πÉ‡∏´‡πâ‡πÇ‡∏ó‡∏£‡∏Å‡∏•‡∏±‡∏ö‡πÑ‡∏õ‡∏™‡∏≠‡∏ö‡∏ñ‡∏≤‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°

    # Keyword-based Features
    df_out['has_line_keyword'] = df_out['message'].str.contains('Line|‡πÑ‡∏•‡∏ô‡πå|‡πÅ‡∏≠‡∏î‡πÑ‡∏•‡∏ô‡πå', case=False, regex=True).astype(int)
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ "Line" ‡∏´‡∏£‡∏∑‡∏≠ "‡πÑ‡∏•‡∏ô‡πå" ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
    # ‡∏Å‡∏≤‡∏£‡∏ä‡∏±‡∏Å‡∏ä‡∏ß‡∏ô‡πÉ‡∏´‡πâ‡πÅ‡∏≠‡∏î‡πÑ‡∏•‡∏ô‡πå‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡πà‡∏≠‡∏á‡∏ó‡∏≤‡∏á‡∏¢‡∏≠‡∏î‡∏ô‡∏¥‡∏¢‡∏°‡∏Ç‡∏≠‡∏á‡∏°‡∏¥‡∏à‡∏â‡∏≤‡∏ä‡∏µ‡∏û‡πÉ‡∏ô‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÑ‡∏ó‡∏¢

    advertisement_keywords = ['‡πÇ‡∏õ‡∏£‡πÇ‡∏°‡∏ä‡∏±‡πà‡∏ô', '‡∏™‡πà‡∏ß‡∏ô‡∏•‡∏î', '‡∏ä‡πâ‡∏≠‡∏õ', '‡∏™‡∏≤‡∏Ç‡∏≤', '‡∏Ñ‡∏•‡∏¥‡∏Å‡πÄ‡∏•‡∏¢', '‡πÅ‡∏ö‡∏£‡∏ô‡∏î‡πå', '‡∏Ñ‡∏≠‡∏•‡πÄ‡∏•‡∏Ñ‡∏ä‡∏±‡πà‡∏ô', '‡πÇ‡∏Ñ‡πâ‡∏î']
    df_out['advertisement_words_count'] = df_out['message'].str.count('|'.join(advertisement_keywords)).astype(int)
    # ‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏≥‡πÇ‡∏Ü‡∏©‡∏ì‡∏≤‡πÉ‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
    # Spam ‡∏°‡∏±‡∏Å‡πÄ‡∏õ‡πá‡∏ô‡πÇ‡∏Ü‡∏©‡∏ì‡∏≤‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏û‡∏∂‡∏á‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå ‡∏Å‡∏≤‡∏£‡∏°‡∏µ‡∏Ñ‡∏≥‡πÄ‡∏´‡∏•‡πà‡∏≤‡∏ô‡∏µ‡πâ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÇ‡∏≠‡∏Å‡∏≤‡∏™‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡πá‡∏ô Spam

    brand_keywords = ['Lazada', 'Shopee', 'AIS', 'dtac', 'True', 'KFC', 'Grab', 'Central', 'Uniqlo', 'SCB', 'KBank']
    df_out['is_known_brand'] = df_out['message'].str.contains('|'.join(brand_keywords), case=False, regex=True).astype(int)
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏ä‡∏∑‡πà‡∏≠‡πÅ‡∏ö‡∏£‡∏ô‡∏î‡πå‡∏ó‡∏µ‡πà‡∏£‡∏π‡πâ‡∏à‡∏±‡∏Å‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
    # ‡∏°‡∏¥‡∏à‡∏â‡∏≤‡∏ä‡∏µ‡∏û‡∏°‡∏±‡∏Å‡πÅ‡∏≠‡∏ö‡∏≠‡πâ‡∏≤‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÅ‡∏ö‡∏£‡∏ô‡∏î‡πå‡∏î‡∏±‡∏á‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏ñ‡∏∑‡∏≠‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏´‡∏•‡∏≠‡∏Å‡∏•‡∏ß‡∏á (Phishing)

    df_out['is_gambling_keyword'] = df_out['message'].str.contains('‡πÄ‡∏ß‡πá‡∏ö‡∏ï‡∏£‡∏á|PG|JOKER|‡∏™‡∏•‡πá‡∏≠‡∏ï|‡∏ö‡∏≤‡∏Ñ‡∏≤‡∏£‡πà‡∏≤|‡∏Ñ‡∏≤‡∏™‡∏¥‡πÇ‡∏ô|‡πÅ‡∏ó‡∏á‡∏´‡∏ß‡∏¢|‡∏ù‡∏≤‡∏Å‡∏ñ‡∏≠‡∏ô|‡∏¢‡∏π‡∏™‡πÉ‡∏´‡∏°‡πà|‡πÅ‡∏ï‡∏Å‡∏á‡πà‡∏≤‡∏¢', case=False, regex=True).astype(int)
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏Ñ‡∏≥‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏û‡∏ô‡∏±‡∏ô‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
    # ‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡∏ö‡πà‡∏á‡∏ä‡∏µ‡πâ‡∏ó‡∏µ‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° Scam ‡∏ó‡∏µ‡πà‡∏ä‡∏±‡∏Å‡∏ä‡∏ß‡∏ô‡πÉ‡∏´‡πâ‡πÄ‡∏•‡πà‡∏ô‡∏Å‡∏≤‡∏£‡∏û‡∏ô‡∏±‡∏ô‡∏≠‡∏≠‡∏ô‡πÑ‡∏•‡∏ô‡πå

    df_out['is_loan_keyword'] = df_out['message'].str.contains('‡∏™‡∏¥‡∏ô‡πÄ‡∏ä‡∏∑‡πà‡∏≠|‡πÄ‡∏á‡∏¥‡∏ô‡∏î‡πà‡∏ß‡∏ô|‡∏≠‡∏ô‡∏∏‡∏°‡∏±‡∏ï‡∏¥‡πÑ‡∏ß|‡∏ß‡∏á‡πÄ‡∏á‡∏¥‡∏ô‡∏Å‡∏π‡πâ|‡∏Å‡∏π‡πâ‡πÑ‡∏î‡πâ|‡πÑ‡∏°‡πà‡πÄ‡∏ä‡πá‡∏Ñ‡∏ö‡∏π‡πÇ‡∏£', case=False, regex=True).astype(int)
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏Ñ‡∏≥‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏™‡∏¥‡∏ô‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
    # ‡πÄ‡∏õ‡πá‡∏ô‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞‡πÄ‡∏î‡πà‡∏ô‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° Scam ‡∏ó‡∏µ‡πà‡∏´‡∏•‡∏≠‡∏Å‡∏•‡∏ß‡∏á‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡πÄ‡∏á‡∏¥‡∏ô‡∏Å‡∏π‡πâ‡∏ô‡∏≠‡∏Å‡∏£‡∏∞‡∏ö‡∏ö

    df_out['is_job_offer_keyword'] = df_out['message'].str.contains('‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏ö‡πâ‡∏≤‡∏ô|‡∏£‡∏≤‡∏¢‡πÑ‡∏î‡πâ‡∏ï‡πà‡∏≠‡∏ß‡∏±‡∏ô|part-time|‡∏û‡∏≤‡∏£‡πå‡∏ó‡πÑ‡∏ó‡∏°‡πå|‡∏£‡∏±‡∏ö‡∏™‡∏°‡∏±‡∏Ñ‡∏£', case=False, regex=True).astype(int)
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏Ñ‡∏≥‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏á‡∏≤‡∏ô‡∏û‡∏≤‡∏£‡πå‡∏ó‡πÑ‡∏ó‡∏°‡πå‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
    # ‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏•‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà‡∏û‡∏ö‡∏ö‡πà‡∏≠‡∏¢‡πÉ‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° Scam ‡∏ó‡∏µ‡πà‡∏´‡∏•‡∏≠‡∏Å‡πÉ‡∏´‡πâ‡∏™‡∏°‡∏±‡∏Ñ‡∏£‡∏á‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡∏à‡∏£‡∏¥‡∏á

    emotional_words = ['‡∏ü‡∏£‡∏µ', '‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•', '‡πÅ‡∏à‡∏Å', '‡πÇ‡∏ö‡∏ô‡∏±‡∏™', '‡πÄ‡∏Ñ‡∏£‡∏î‡∏¥‡∏ï', '‡∏£‡∏±‡∏ö‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡πå', '‡πÄ‡∏á‡∏¥‡∏ô‡∏Ñ‡∏∑‡∏ô', '‡∏î‡πà‡∏ß‡∏ô', '‡∏†‡∏≤‡∏¢‡πÉ‡∏ô', '‡∏à‡∏≥‡∏Å‡∏±‡∏î', '‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô', '‡∏ú‡∏¥‡∏î‡∏õ‡∏Å‡∏ï‡∏¥', '‡∏ñ‡∏π‡∏Å‡∏£‡∏∞‡∏á‡∏±‡∏ö', '‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤']
    df_out['emotional_words_count'] = df_out['message'].str.count('|'.join(emotional_words)).astype(int)
    # ‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏≥‡∏Å‡∏£‡∏∞‡∏ï‡∏∏‡πâ‡∏ô‡∏≠‡∏≤‡∏£‡∏°‡∏ì‡πå‡πÉ‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
    # Spam/Scam ‡∏°‡∏±‡∏Å‡πÉ‡∏ä‡πâ‡∏Ñ‡∏≥‡πÄ‡∏´‡∏•‡πà‡∏≤‡∏ô‡∏µ‡πâ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏™‡∏∂‡∏Å‡πÄ‡∏£‡πà‡∏á‡∏£‡∏µ‡∏ö, ‡πÇ‡∏•‡∏†, ‡∏´‡∏£‡∏∑‡∏≠‡∏Å‡∏•‡∏±‡∏ß ‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏ú‡∏π‡πâ‡∏£‡∏±‡∏ö‡∏Ç‡∏≤‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏∞‡∏°‡∏±‡∏î‡∏£‡∏∞‡∏ß‡∏±‡∏á

    obfuscated_pattern = r'\\b[‡∏Å-‡∏Æ]*[a-zA-Z0-9]+[‡∏Å-‡∏Æ]+[a-zA-Z0-9]*\\b'
    df_out['obfuscated_word_count'] = df_out['message'].str.count(obfuscated_pattern)
    # ‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡∏ú‡∏™‡∏°‡∏Å‡∏±‡∏ô‡∏ú‡∏¥‡∏î‡∏õ‡∏Å‡∏ï‡∏¥ (‡πÄ‡∏ä‡πà‡∏ô ‡∏üs‡∏µ, ‡πÇu‡∏ô‡∏±‡∏™)
    # ‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ‡∏ó‡∏µ‡πà‡∏°‡∏¥‡∏à‡∏â‡∏≤‡∏ä‡∏µ‡∏û‡πÉ‡∏ä‡πâ‡∏ö‡πà‡∏≠‡∏¢‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏•‡∏ö‡πÄ‡∏•‡∏µ‡πà‡∏¢‡∏á‡∏£‡∏∞‡∏ö‡∏ö‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏Ñ‡∏≥ (Keyword Filtering) ‡∏ã‡∏∂‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏™‡∏±‡∏ç‡∏ç‡∏≤‡∏ì‡∏Ç‡∏≠‡∏á Scam ‡∏ó‡∏µ‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô

    # ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° (‡∏ï‡πâ‡∏≠‡∏á‡∏ó‡∏≥‡∏Å‡πà‡∏≠‡∏ô‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Word Diversity)
    df_out['processed_message'] = df_out['message'].apply(text_process)

    # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≥‡∏®‡∏±‡∏û‡∏ó‡πå
    df_out['word_diversity_ratio'] = df_out['processed_message'].apply(
        lambda x: len(set(x.split())) / (len(x.split()) + 1e-6) # +1e-6 ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏´‡∏≤‡∏£‡∏î‡πâ‡∏ß‡∏¢‡∏®‡∏π‡∏ô‡∏¢‡πå
    )
    # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏™‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ã‡πâ‡∏≥‡∏Å‡∏±‡∏ô‡∏ï‡πà‡∏≠‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏≥‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
    # ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° Spam ‡∏°‡∏±‡∏Å‡πÉ‡∏ä‡πâ‡∏Ñ‡∏≥‡∏ã‡πâ‡∏≥‡πÜ (‡πÄ‡∏ä‡πà‡∏ô ‡∏ü‡∏£‡∏µ ‡∏ü‡∏£‡∏µ ‡∏ü‡∏£‡∏µ) ‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≥‡∏®‡∏±‡∏û‡∏ó‡πå‡∏ï‡πà‡∏≥

    return df_out


print("\n‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á Features")

if 'df' in locals():
    df_featured = create_features(df)
    print("\n‡∏™‡∏£‡πâ‡∏≤‡∏á Features ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")
else:
    print("\n‡πÑ‡∏°‡πà‡∏û‡∏ö DataFrame")

print("‡∏Å‡∏≥‡∏´‡∏ô‡∏î Labels (Label Definition)")

if 'df_featured' in locals():
    label_encoder = LabelEncoder()
    df_featured['label_encoded'] = label_encoder.fit_transform(df_featured['label'])
    print("‡πÅ‡∏õ‡∏•‡∏á Label ‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢")

    encoder_path = os.path.join(DRIVE_PROJECT_PATH, 'label_encoder.joblib')
    joblib.dump(label_encoder, encoder_path)  # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ï‡∏±‡∏ß label_encoder ‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏ü‡∏•‡πå .joblib ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏ß‡πâ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô
    print(f"\n‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å Label Encoder ‡∏•‡∏á Drive ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")
else:
    print("‡πÑ‡∏°‡πà‡∏û‡∏ö DataFrame")

"""# ‡∏ó‡∏î‡∏•‡∏≠‡∏á‡∏´‡∏≤‡∏™‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô‡πÅ‡∏•‡∏∞‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î"""

print("‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÅ‡∏•‡∏∞‡∏ó‡∏î‡∏•‡∏≠‡∏á‡∏ó‡∏µ‡πà‡∏™‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô 60:40")

# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ DataFrame
if 'df_featured' in locals():
    # ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤
    # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Input (X) ‡πÅ‡∏•‡∏∞ Output (y)
    X = df_featured['processed_message']
    y = df_featured['label_encoded']

    # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÉ‡∏ä‡πâ
    models = {
        "Logistic Regression": LogisticRegression(max_iter=1000, random_state=42),
        "Random Forest": RandomForestClassifier(random_state=42),
        "Neural Network": MLPClassifier(random_state=42, max_iter=500),
        "Naive Bayes": MultinomialNB()
    }

    # ‡∏™‡∏£‡πâ‡∏≤‡∏á Dictionary ‡πÄ‡∏Å‡πá‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
    experiment_results = {}

    # ‡∏ó‡∏î‡∏•‡∏≠‡∏á‡∏ó‡∏µ‡πà‡∏™‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô 60:40
    print("\n‡∏ó‡∏î‡∏•‡∏≠‡∏á‡∏ó‡∏µ‡πà‡∏™‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô 60:40 ")
    ratio_key = "60:40"
    experiment_results[ratio_key] = {}

    # ‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏•‡∏∞‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô TF-IDF
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.4, random_state=42, stratify=y)
    tfidf_vectorizer = TfidfVectorizer(tokenizer=word_tokenize)
    X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)
    X_test_tfidf = tfidf_vectorizer.transform(X_test)

    # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏ó‡∏∏‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•
    for model_name, model in models.items():
        model.fit(X_train_tfidf, y_train)
        y_pred = model.predict(X_test_tfidf)
        accuracy = accuracy_score(y_test, y_pred)
        experiment_results[ratio_key][model_name] = accuracy
        print(f"  - ‡πÇ‡∏°‡πÄ‡∏î‡∏• : {model_name:<15} | Accuracy : {accuracy:.5f}")

else:
    print("‡πÑ‡∏°‡πà‡∏û‡∏ö DataFrame")

print("\n‡∏ó‡∏î‡∏•‡∏≠‡∏á‡∏ó‡∏µ‡πà‡∏™‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô 70:30")

# ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡∏µ‡∏¢‡πå‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏Å‡πá‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
ratio_key = "70:30"
experiment_results[ratio_key] = {}

# ‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏•‡∏∞‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô TF-IDF
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42, stratify=y)
tfidf_vectorizer = TfidfVectorizer(tokenizer=word_tokenize)
X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)
X_test_tfidf = tfidf_vectorizer.transform(X_test)

# ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•
for model_name, model in models.items():
    model.fit(X_train_tfidf, y_train)
    y_pred = model.predict(X_test_tfidf)
    accuracy = accuracy_score(y_test, y_pred)
    experiment_results[ratio_key][model_name] = accuracy
    print(f"  - ‡πÇ‡∏°‡πÄ‡∏î‡∏• : {model_name:<15} | Accuracy: {accuracy:.5f}")

print("\n‡∏ó‡∏î‡∏•‡∏≠‡∏á‡∏ó‡∏µ‡πà‡∏™‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô 80:20")

# ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡∏µ‡∏¢‡πå‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏Å‡πá‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
ratio_key = "80:20"
experiment_results[ratio_key] = {}

# ‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏•‡∏∞‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô TF-IDF
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y)
tfidf_vectorizer = TfidfVectorizer(tokenizer=word_tokenize)
X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)
X_test_tfidf = tfidf_vectorizer.transform(X_test)

# ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•
for model_name, model in models.items():
    model.fit(X_train_tfidf, y_train)
    y_pred = model.predict(X_test_tfidf)
    accuracy = accuracy_score(y_test, y_pred)
    experiment_results[ratio_key][model_name] = accuracy
    print(f"  - ‡πÇ‡∏°‡πÄ‡∏î‡∏• : {model_name:<15} | Accuracy: {accuracy:.5f}")

"""## ‡∏™‡∏£‡∏∏‡∏õ
‡∏™‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô‡πÅ‡∏•‡∏∞‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà ‡∏Ñ‡πà‡∏≤ Accuracy ‡πÄ‡∏¢‡∏≠‡∏∞‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏Ñ‡∏∑‡∏≠ ‡∏™‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô 70:30 ‡πÇ‡∏°‡πÄ‡∏î‡∏• Neural Network ‡∏Ñ‡πà‡∏≤ Accuracy ‡∏≠‡∏¢‡∏π‡πà‡∏ó‡∏µ‡πà 0.92785

# ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Train ‡πÅ‡∏•‡∏∞ Test
"""

if 'df_featured' in locals():
    X = df_featured['processed_message']
    y = df_featured['label_encoded']

    # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å test_size ‡πÄ‡∏õ‡πá‡∏ô 0.3 ‡∏ï‡∏≤‡∏°‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏•‡∏≠‡∏á‡∏™‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=42, stratify=y)
    print(f"‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏õ‡πá‡∏ô Train: {len(X_train)} ‡πÅ‡∏ñ‡∏ß, Test: {len(X_test)} ‡πÅ‡∏ñ‡∏ß (‡∏™‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô 70:30)")

    # ‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏î‡πâ‡∏ß‡∏¢ TF-IDF
    tfidf_vectorizer = TfidfVectorizer(tokenizer=word_tokenize)
    X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)
    X_test_tfidf = tfidf_vectorizer.transform(X_test)

    print("\n‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô TF-IDF ‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢")
else:
    print("‡πÑ‡∏°‡πà‡∏û‡∏ö DataFrame 'df_featured'")

"""# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•
‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏• Logistic Regression,Random Forest,Neural Network,Naive Bayes
‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Classification Report ‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ï‡∏±‡∏ß
"""

models = {
   "Logistic Regression": LogisticRegression(max_iter=1000, random_state=42),
    "Random Forest": RandomForestClassifier(random_state=42),
    "Neural Network": MLPClassifier(random_state=42, max_iter=500),
    "Naive Bayes": MultinomialNB()
}

results = {}  # ‡∏™‡∏£‡πâ‡∏≤‡∏á dictionary ‡∏ß‡πà‡∏≤‡∏á

# ‡∏™‡∏≠‡∏ô‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏•‡∏ó‡∏∏‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•
for model_name, model in models.items():
    print(f"\n‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏≠‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏• : {model_name}")
    model.fit(X_train_tfidf, y_train)
    y_pred = model.predict(X_test_tfidf)
    accuracy = accuracy_score(y_test, y_pred)
    results[model_name] = accuracy

    # ‡πÅ‡∏™‡∏î‡∏á‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô
    print(f"Accuracy : {accuracy:.4f}")
    print("\nClassification Report :")
    # ‡πÉ‡∏ä‡πâ target_names=label_encoder.classes_ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏™‡∏î‡∏á‡∏ä‡∏∑‡πà‡∏≠ Label ‡πÅ‡∏ó‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç
    print(classification_report(y_test, y_pred, target_names=label_encoder.classes_, zero_division=0))

#‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•
best_model_name = max(results, key=results.get)
print(f"‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏•‡∏≠‡∏á‡∏Ñ‡∏∑‡∏≠ : -- {best_model_name} --")
print(f"   ‡∏î‡πâ‡∏ß‡∏¢ Accuracy ‡∏£‡∏ß‡∏° = {results[best_model_name]:.4f}")

"""# ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏à‡∏π‡∏ô‡πÑ‡∏Æ‡πÄ‡∏õ‡∏≠‡∏£‡πå‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå (Hyperparameter Tuning)"""

# ‡∏õ‡∏£‡∏±‡∏ö‡∏à‡∏π‡∏ô Hyperparameter ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Neural Network
# ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ (Parameter Grid)
param_grid_nn = {
    'hidden_layer_sizes': [(50,), (100,), (50, 25)],
    'activation': ['tanh', 'relu'],
    'solver': ['adam'],
    'alpha': [0.0001, 0.05],
    'learning_rate': ['constant', 'adaptive'],
}

# ‡∏™‡∏£‡πâ‡∏≤‡∏á GridSearchCV ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î
grid_search = GridSearchCV(
    MLPClassifier(random_state=42, max_iter=500),  # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏• Neural Network ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏≥‡∏´‡∏ô‡∏î max_iter
    param_grid_nn,
    cv=3,
    verbose=2,
    n_jobs=-1
)

# ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÅ‡∏•‡∏∞‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏î‡πâ‡∏ß‡∏¢‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• TF-IDF ‡∏ó‡∏µ‡πà‡πÅ‡∏õ‡∏•‡∏á‡πÅ‡∏•‡πâ‡∏ß
grid_search.fit(X_train_tfidf, y_train)

# ‡∏î‡∏∂‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏à‡∏≤‡∏Å‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤
best_tuned_model = grid_search.best_estimator_

print(f"\nHyperparameters ‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î: {grid_search.best_params_}")
print("‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏ñ‡∏π‡∏Å‡πÄ‡∏Å‡πá‡∏ö‡πÉ‡∏ô‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£ 'best_tuned_model'")

# ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏±‡∏ö‡∏à‡∏π‡∏ô‡πÅ‡∏•‡πâ‡∏ß
# ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏ñ‡∏µ‡∏¢‡∏£‡∏î‡πâ‡∏ß‡∏¢ 5-Fold Cross-Validation ‡∏ö‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Train
cv_scores = cross_val_score(best_tuned_model, X_train_tfidf, y_train, cv=5)
print(f"\nAccuracy ‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢ (Cross-Validation) : {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.2f})")

# ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡∏ö‡∏ô Test Set (‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÄ‡∏Ñ‡∏¢‡πÄ‡∏´‡πá‡∏ô)
y_pred_final = best_tuned_model.predict(X_test_tfidf)

# ‡πÅ‡∏™‡∏î‡∏á Classification Report
print("\nClassification Report ‡∏ö‡∏ô Test Set:\n")
print(classification_report(y_test, y_pred_final, target_names=label_encoder.classes_))

"""# ‡∏™‡∏£‡πâ‡∏≤‡∏á Pipeline ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• ‡πÅ‡∏•‡∏∞‡∏ó‡∏î‡∏™‡∏≠‡∏ö"""

print("‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏™‡∏£‡πâ‡∏≤‡∏á Pipeline...")

# ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Input (X) ‡πÅ‡∏•‡∏∞ Output (y)
# ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå ‡πÑ‡∏°‡πà‡∏£‡∏ß‡∏° label ‡πÅ‡∏•‡∏∞‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏î‡∏¥‡∏ö
feature_columns = [
    'processed_message', 'message_length', 'digit_count', 'uppercase_ratio',
    'repeated_punctuation_count', 'url_count', 'has_short_url', 'has_phone',
    'has_line_keyword', 'advertisement_words_count', 'is_known_brand',
    'is_gambling_keyword', 'is_loan_keyword', 'is_job_offer_keyword',
    'emotional_words_count', 'obfuscated_word_count', 'word_diversity_ratio'
]
X = df_featured[feature_columns]
y = df_featured['label_encoded']
print(f"‡πÉ‡∏ä‡πâ‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î {len(feature_columns)} ‡∏ï‡∏±‡∏ß‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•")

# ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Ç‡∏≠‡∏á‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå
text_feature = 'processed_message'
numeric_features = [col for col in X.columns if col != text_feature]
print(f"‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° : '{text_feature}'")
print(f"‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç : {len(numeric_features)} ‡∏ï‡∏±‡∏ß")

# ‡∏™‡∏£‡πâ‡∏≤‡∏á Preprocessor ‡∏î‡πâ‡∏ß‡∏¢ ColumnTransformer
# ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡∏£‡∏ß‡∏°‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏•‡∏∞‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡πÄ‡∏Ç‡πâ‡∏≤‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏±‡∏ô
preprocessor = ColumnTransformer(
    transformers=[
        # Pipeline ‡∏¢‡πà‡∏≠‡∏¢‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
        ('tfidf', TfidfVectorizer(tokenizer=word_tokenize), text_feature),
        # Pipeline ‡∏¢‡πà‡∏≠‡∏¢‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç
        ('numeric', StandardScaler(), numeric_features)
    ],
    remainder='passthrough'
)
print("‡∏™‡∏£‡πâ‡∏≤‡∏á Preprocessor ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏£‡∏ß‡∏°‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")

# ‡∏™‡∏£‡πâ‡∏≤‡∏á Pipeline ‡∏ó‡∏µ‡πà‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î
# ‡πÉ‡∏ä‡πâ ImbPipeline ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ SMOTE ‡πÅ‡∏•‡∏∞‡πÉ‡∏ä‡πâ best_tuned_model
final_production_pipeline = ImbPipeline([
    ('preprocessor', preprocessor),          # ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÅ‡∏•‡∏∞‡∏£‡∏ß‡∏°‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
    ('smote', SMOTE(random_state=42)),       # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏±‡∏á‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ Imbalance
    ('clf', best_tuned_model)                # ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ú‡∏•‡∏î‡πâ‡∏ß‡∏¢ Neural Network ‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏±‡∏ö‡∏à‡∏π‡∏ô‡πÅ‡∏•‡πâ‡∏ß
])
print("‡∏™‡∏£‡πâ‡∏≤‡∏á Pipeline ‡∏ó‡∏µ‡πà‡∏£‡∏ß‡∏°‡∏ó‡∏∏‡∏Å‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå")

# ‡∏™‡∏≠‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏î‡πâ‡∏ß‡∏¢‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
print("‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏≠‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏î‡πâ‡∏ß‡∏¢‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î...")
final_production_pipeline.fit(X, y)
print("‡∏™‡∏≠‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")

# ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å Pipeline
# ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏´‡∏°‡πà‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡∏ó‡∏±‡∏ö‡∏Ç‡∏≠‡∏á‡πÄ‡∏î‡∏¥‡∏°
pipeline_path_final = os.path.join(DRIVE_PROJECT_PATH, 'final_sms_classifier_production.joblib')
joblib.dump(final_production_pipeline, pipeline_path_final)
print(f"‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å Pipeline ‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô Production ‡∏•‡∏á‡∏ó‡∏µ‡πà '{pipeline_path_final}' ‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢‡πÅ‡∏•‡πâ‡∏ß")

def analyze_and_log_sms(sms_text, pipeline_path, encoder_path, log_file_path):
    # ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÅ‡∏•‡∏∞ Encoder
    pipeline = joblib.load(pipeline_path)
    le = joblib.load(encoder_path)

    # ‡∏™‡∏£‡πâ‡∏≤‡∏á DataFrame ‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏ö‡πÄ‡∏Ç‡πâ‡∏≤‡∏°‡∏≤
    input_df = pd.DataFrame([{'message': sms_text}])

    # ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô create_features ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå
    features_df = create_features(input_df)

    # ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ú‡∏•‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ DataFrame ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå
    prediction_encoded = pipeline.predict(features_df)[0]
    prediction = le.inverse_transform([prediction_encoded])[0]

    # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô predict_proba ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö LIME ‡πÉ‡∏´‡πâ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ create_features ‡∏î‡πâ‡∏ß‡∏¢
    def lime_predict_fn(texts):
        lime_df = create_features(pd.DataFrame(texts, columns=['message']))
        return pipeline.predict_proba(lime_df)

    # ‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏î‡πâ‡∏ß‡∏¢ LIME
    reason_string = "N/A"
    try:
        explainer = lime.lime_text.LimeTextExplainer(class_names=le.classes_, bow=False, split_expression=word_tokenize)
        exp = explainer.explain_instance(
            sms_text,
            lime_predict_fn,
            num_features=5,
            labels=[prediction_encoded]
        )
        reason_parts = [f"'{word} : {weight}'" for word, weight in exp.as_list(label=prediction_encoded) if weight > 0]
        reason_string = ", ".join(reason_parts) if reason_parts else "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•"
    except Exception as e:
        reason_string = f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô LIME : {e}"

    # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å Log
    try:
        file_exists = os.path.exists(log_file_path)
        with open(log_file_path, 'a', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            if not file_exists:
                writer.writerow(["timestamp", "prediction", "message", "reason"])
            writer.writerow([datetime.now().strftime("%Y-%m-%d %H:%M:%S"), prediction, sms_text, reason_string])
    except Exception as e:
        print(f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å Log : {e}")

    return prediction, reason_string

print("‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")

# ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡∏Ç‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÉ‡∏ô‡πÄ‡∏ã‡∏•‡∏•‡πå‡∏ô‡∏µ‡πâ
DRIVE_PROJECT_PATH = '/content/drive/MyDrive/SMS_Scam_Final_Project/'
pipeline_path = os.path.join(DRIVE_PROJECT_PATH, 'final_sms_classifier_production.joblib')
encoder_path = os.path.join(DRIVE_PROJECT_PATH, 'label_encoder.joblib')
LOG_FILE = os.path.join(DRIVE_PROJECT_PATH, 'prediction_log.csv')

print("‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏£‡∏∞‡∏ö‡∏ö")

sms_to_test = "‡πÇ‡∏ó‡∏£‡∏î‡πà‡∏ß‡∏ô *191# ‡πÄ‡∏û‡∏µ‡∏¢‡∏á 3‡∏ö/‡∏ô‡∏≤‡∏ó‡∏µ! "

# ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô‡πÉ‡∏´‡∏°‡πà ‡πÇ‡∏î‡∏¢‡∏™‡πà‡∏á path ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡∏î‡πâ‡∏ß‡∏¢
pred, reason = analyze_and_log_sms(sms_to_test, pipeline_path, encoder_path, LOG_FILE)

print(f"‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° : '{sms_to_test}'")
print(f" -> ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢ : {pred}")
print(f" -> ‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•‡∏´‡∏•‡∏±‡∏Å : {reason}")
print(f"\n‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•‡∏•‡∏á '{LOG_FILE}' ‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢")

"""# ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•"""

test_dataset = [
    # ‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó : OK
    {'message': '‡∏û‡∏±‡∏™‡∏î‡∏∏‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì TH123456789 ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏à‡∏∞‡∏à‡∏±‡∏î‡∏™‡πà‡∏á‡∏ñ‡∏∂‡∏á‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏¢‡πÉ‡∏ô‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ', 'label': 'OK'},
    {'message': '‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡∏ã‡∏∑‡πâ‡∏≠ #98765 ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏à‡∏±‡∏î‡∏™‡πà‡∏á‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß ‡∏Ç‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£', 'label': 'OK'},
    {'message': '‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏Ñ‡∏∏‡∏ì‡∏™‡∏°‡∏ä‡∏≤‡∏¢ ‡∏Ç‡∏ì‡∏∞‡∏ô‡∏µ‡πâ‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏¥‡∏î‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏£‡∏∞‡∏ö‡∏ö ‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢‡πÉ‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡πÑ‡∏°‡πà‡∏™‡∏∞‡∏î‡∏ß‡∏Å', 'label': 'OK'},
    {'message': '‡∏Ñ‡πà‡∏≤‡∏ô‡πâ‡∏≥‡∏õ‡∏£‡∏∞‡∏õ‡∏≤‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡∏Ñ‡∏∑‡∏≠ 350 ‡∏ö‡∏≤‡∏ó ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏ä‡∏≥‡∏£‡∏∞‡∏†‡∏≤‡∏¢‡πÉ‡∏ô‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà 15', 'label': 'OK'},
    {'message': '‡∏ú‡∏•‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏∏‡∏Ç‡∏†‡∏≤‡∏û‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏≠‡∏≠‡∏Å‡πÅ‡∏•‡πâ‡∏ß ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏î‡∏π‡πÑ‡∏î‡πâ‡∏ó‡∏µ‡πà‡πÅ‡∏≠‡∏õ‡πÇ‡∏£‡∏á‡∏û‡∏¢‡∏≤‡∏ö‡∏≤‡∏•', 'label': 'OK'},

    # ‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó : OTP
    {'message': '‡∏£‡∏´‡∏±‡∏™ OTP ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠ 882154 ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£', 'label': 'OTP'},
    {'message': 'Lazada: 123456 ‡∏Ñ‡∏∑‡∏≠‡∏£‡∏´‡∏±‡∏™‡∏¢‡∏∑‡∏ô‡∏¢‡∏±‡∏ô‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì (ref: ABC)', 'label': 'OTP'},
    {'message': '[‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏Å‡∏£‡∏∏‡∏á‡πÑ‡∏ó‡∏¢] ‡∏£‡∏´‡∏±‡∏™ OTP ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠ 604831 ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏≥‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ Krungthai NEXT', 'label': 'OTP'},
    {'message': '(SCB) OTP: 901-222 ‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡∏°‡∏±‡∏Ñ‡∏£‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£', 'label': 'OTP'},
    {'message': 'Google: G-587123 ‡∏Ñ‡∏∑‡∏≠‡∏£‡∏´‡∏±‡∏™‡∏¢‡∏∑‡∏ô‡∏¢‡∏±‡∏ô‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì', 'label': 'OTP'},

    # ‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó : scam
    {'message': '‡∏ö‡∏±‡∏ç‡∏ä‡∏µ‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏ñ‡∏π‡∏Å‡∏£‡∏∞‡∏á‡∏±‡∏ö ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏Ñ‡∏•‡∏¥‡∏Å line.ee/xyz ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏¢‡∏∑‡∏ô‡∏¢‡∏±‡∏ô‡∏ï‡∏±‡∏ß‡∏ï‡∏ô', 'label': 'scam'},
    {'message': '‡∏Ñ‡∏∏‡∏ì‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡πå‡∏™‡∏¥‡∏ô‡πÄ‡∏ä‡∏∑‡πà‡∏≠ 50,000 ‡∏ö‡∏≤‡∏ó ‡∏≠‡∏ô‡∏∏‡∏°‡∏±‡∏ï‡∏¥‡πÉ‡∏ô 5 ‡∏ô‡∏≤‡∏ó‡∏µ ‡πÑ‡∏°‡πà‡πÄ‡∏ä‡πá‡∏Ñ‡∏ö‡∏π‡πÇ‡∏£ ‡πÅ‡∏≠‡∏î‡πÑ‡∏•‡∏ô‡πå @loanfast', 'label': 'scam'},
    {'message': '‡∏´‡∏≤‡∏£‡∏≤‡∏¢‡πÑ‡∏î‡πâ‡πÄ‡∏™‡∏£‡∏¥‡∏°‡∏á‡πà‡∏≤‡∏¢‡πÜ ‡∏ß‡∏±‡∏ô‡∏•‡∏∞ 800-1000 ‡∏ö‡∏≤‡∏ó ‡πÅ‡∏Ñ‡πà‡∏Å‡∏î‡∏î‡∏π‡∏¢‡∏π‡∏ó‡∏π‡∏õ ‡∏™‡∏ô‡πÉ‡∏à‡πÅ‡∏≠‡∏î‡πÑ‡∏•‡∏ô‡πå', 'label': 'scam'},
    {'message': '‡∏Ñ‡∏∏‡∏ì‡∏°‡∏µ‡∏û‡∏±‡∏™‡∏î‡∏∏‡∏à‡∏≤‡∏Å DHL ‡∏ï‡∏Å‡∏Ñ‡πâ‡∏≤‡∏á‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡πÑ‡∏°‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô ‡πÇ‡∏õ‡∏£‡∏î‡∏¢‡∏∑‡∏ô‡∏¢‡∏±‡∏ô‡∏ó‡∏µ‡πà cutt.ly/update', 'label': 'scam'},
    {'message': '‡∏ö‡∏±‡∏ç‡∏ä‡∏µ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ú‡∏¥‡∏î‡∏õ‡∏Å‡∏ï‡∏¥ ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏¢‡∏∑‡∏ô‡∏¢‡∏±‡∏ô‡∏ï‡∏±‡∏ß‡∏ï‡∏ô‡∏î‡πà‡∏ß‡∏ô >> www.bkk-safe-verify.com', 'label': 'scam'},

    # ‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó : spam
    {'message': 'FLASH SALE! ‡πÄ‡∏™‡∏∑‡πâ‡∏≠‡∏ú‡πâ‡∏≤‡πÅ‡∏ö‡∏£‡∏ô‡∏î‡πå‡∏•‡∏î 70% ‡∏ß‡∏±‡∏ô‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô ‡∏ä‡πâ‡∏≠‡∏õ‡πÄ‡∏•‡∏¢', 'label': 'spam'},
    {'message': '‡∏ä‡πâ‡∏≠‡∏õ Lazada Shopee ‡∏•‡∏î‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î 90% ‡∏ó‡∏±‡∏Å‡πÄ‡∏•‡∏¢ LINE: @shop-now', 'label': 'spam'},
    {'message': '‡πÇ‡∏õ‡∏£‡∏™‡∏∏‡∏î‡∏Ñ‡∏∏‡πâ‡∏°! ‡πÄ‡∏ô‡πá‡∏ï‡πÑ‡∏°‡πà‡∏≠‡∏±‡πâ‡∏ô ‡πÇ‡∏ó‡∏£‡∏ü‡∏£‡∏µ‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏ß‡∏±‡∏ô‡∏•‡∏∞ 9 ‡∏ö‡∏≤‡∏ó ‡∏™‡∏°‡∏±‡∏Ñ‡∏£‡∏Å‡∏î *123#', 'label': 'spam'},
    {'message': '‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏≠‡∏õ‡∏î‡∏π‡∏î‡∏ß‡∏á‡∏ü‡∏£‡∏µ ‡πÅ‡∏°‡πà‡∏ô‡∏Ç‡∏±‡πâ‡∏ô‡πÄ‡∏ó‡∏û ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Ñ‡∏≥‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ñ‡∏∏‡∏ì >> www.horolive.today', 'label': 'spam'},
    {'message': '‡∏î‡∏π‡∏´‡∏ô‡∏±‡∏á‡∏ü‡∏£‡∏µ‡πÑ‡∏°‡πà‡∏≠‡∏±‡πâ‡∏ô‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡πÅ‡∏£‡∏Å ‡∏™‡∏°‡∏±‡∏Ñ‡∏£‡πÄ‡∏•‡∏¢‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ', 'label': 'spam'},
]

print(f"‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô {len(test_dataset)} ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°")

# ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡∏Ç‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
DRIVE_PROJECT_PATH = '/content/drive/MyDrive/SMS_Scam_Final_Project/'
pipeline_path = os.path.join(DRIVE_PROJECT_PATH, 'final_sms_classifier_production.joblib')
encoder_path = os.path.join(DRIVE_PROJECT_PATH, 'label_encoder.joblib')
LOG_FILE = os.path.join(DRIVE_PROJECT_PATH, 'prediction_log.csv')

# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏î‡∏™‡∏≠‡∏ö
if 'test_dataset' in locals():
    true_labels = []
    predicted_labels = []

    print("\n‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏£‡∏∞‡∏ö‡∏ö‡πÅ‡∏ö‡∏ö‡∏Å‡∏•‡∏∏‡πà‡∏°")

    # ‡∏ß‡∏ô‡∏•‡∏π‡∏õ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏ó‡∏µ‡∏•‡∏∞‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
    for item in test_dataset:
        sms_text = item['message']
        true_label = item['label']

        # ‡∏™‡∏£‡πâ‡∏≤‡∏á DataFrame ‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏∂‡∏á‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô
        temp_df = pd.DataFrame([item])
        features_df = create_features(temp_df)

        # ‡∏î‡∏∂‡∏á‡∏Ñ‡πà‡∏≤‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡∏≠‡∏≠‡∏Å‡∏°‡∏≤
        features_series = features_df.drop(columns=['message', 'label', 'processed_message'], errors='ignore').iloc[0]

        # ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏´‡∏•‡∏±‡∏Å ‡πÇ‡∏î‡∏¢‡∏™‡πà‡∏á‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡∏î‡πâ‡∏ß‡∏¢
        pred_label, reason = analyze_and_log_sms(sms_text, pipeline_path, encoder_path, LOG_FILE)

        # ‡πÄ‡∏Å‡πá‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ô‡∏≥‡πÑ‡∏õ‡∏™‡∏£‡πâ‡∏≤‡∏á Report
        true_labels.append(true_label)
        predicted_labels.append(pred_label)

        # ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•
        print(f"\n‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°  : '{sms_text}'")
        print(f" -> ‡∏ú‡∏•‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢ : {pred_label} (‡∏Ñ‡πà‡∏≤‡∏à‡∏£‡∏¥‡∏á: {true_label})")

        # ‡πÅ‡∏™‡∏î‡∏á‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡∏ó‡∏µ‡πà‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡∏∂‡πâ‡∏ô
        print(" -> ‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡∏ó‡∏µ‡πà‡∏™‡∏£‡πâ‡∏≤‡∏á:")
        for col, val in features_series.items():
            print(f"    - {col}: {val}")

        print(f" -> ‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•‡∏´‡∏•‡∏±‡∏Å : {reason}")

    # ‡πÅ‡∏™‡∏î‡∏á‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô
    print("\n\n‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö Classification Report")
    le = joblib.load(encoder_path)
    print(classification_report(true_labels, predicted_labels, target_names=le.classes_))

else:
    print("‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠ 'test_dataset'")

"""# HTML ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏£‡∏∞‡∏ö‡∏ö"""

!pip install flask pandas pyngrok pythainlp lime scikit-learn imbalanced-learn -q

!ngrok config add-authtoken ..............................................

import os
from google.colab import drive

print("‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ Drive")
drive.mount('/content/drive', force_remount=True)

DRIVE_PROJECT_PATH = '/content/drive/MyDrive/SMS_Scam_Final_Project/'
TEMPLATE_DIR = os.path.join(DRIVE_PROJECT_PATH, 'templates')
html_file_path = os.path.join(TEMPLATE_DIR, 'index.html')

os.makedirs(TEMPLATE_DIR, exist_ok=True)
print(f"‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à‡∏ó‡∏µ‡πà '{TEMPLATE_DIR}' ")

# Commented out IPython magic to ensure Python compatibility.
# %%writefile /content/drive/MyDrive/SMS_Scam_Final_Project/templates/index.html
# <!DOCTYPE html>
# <html lang="th">
# <head>
#     <meta charset="UTF-8">
#     <meta name="viewport" content="width=device-width, initial-scale=1.0">
#     <title>SMS Analyzer - ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°</title>
#     <link href="https://fonts.googleapis.com/css2?family=Kanit:wght@300;400;500;700&display=swap" rel="stylesheet">
#     <link href="https://unpkg.com/aos@2.3.1/dist/aos.css" rel="stylesheet">
#     <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
#     <style>
#         :root {
#             --bg-dark: #121418;
#             --bg-medium: #1a1c22;
#             --bg-light: #2c2f3a;
#             --text-primary: #f0f0f0;
#             --text-secondary: #a0a0a0;
#             --accent-primary: #4f46e5;
#             --accent-secondary: #7c3aed;
#             --border-color: rgba(255, 255, 255, 0.1);
#             --success-color: #22c55e;
#             --warning-color: #f59e0b;
#             --danger-color: #ef4444;
#             --info-color: #3b82f6;
#         }
# 
#         * { box-sizing: border-box; }
# 
#         body {
#             font-family: 'Kanit', sans-serif;
#             background-color: var(--bg-dark);
#             color: var(--text-primary);
#             margin: 0;
#             padding: 2rem 1rem;
#             display: flex;
#             flex-direction: column;
#             align-items: center;
#             min-height: 100vh;
#             background-image: radial-gradient(circle at 1px 1px, rgba(255,255,255,0.05) 1px, transparent 0);
#             background-size: 20px 20px;
#         }
# 
#         .container {
#             width: 100%;
#             max-width: 800px;
#             background: rgba(26, 28, 34, 0.8);
#             backdrop-filter: blur(12px);
#             padding: 2rem;
#             border-radius: 24px;
#             border: 1px solid var(--border-color);
#             box-shadow: 0 10px 40px rgba(0, 0, 0, 0.5);
#         }
# 
#         .header {
#             text-align: center;
#             margin-bottom: 2.5rem;
#         }
#         .header h1 {
#             font-size: 2.8rem;
#             font-weight: 700;
#             margin: 0;
#             background: -webkit-linear-gradient(45deg, var(--accent-secondary), var(--accent-primary));
#             -webkit-background-clip: text;
#             -webkit-text-fill-color: transparent;
#         }
#         .header .tagline {
#             font-size: 1.1rem;
#             color: var(--text-secondary);
#             margin-top: 0.5rem;
#         }
# 
#         textarea {
#             font-family: 'Kanit', sans-serif;
#             font-size: 1rem;
#             width: 100%;
#             min-height: 150px;
#             padding: 1rem;
#             border-radius: 12px;
#             border: 1px solid var(--bg-light);
#             background-color: var(--bg-medium);
#             color: var(--text-primary);
#             resize: vertical;
#             transition: all 0.3s ease;
#         }
#         textarea:focus {
#             outline: none;
#             border-color: var(--accent-primary);
#             box-shadow: 0 0 0 3px rgba(79, 70, 229, 0.5);
#         }
# 
#         button {
#             font-family: 'Kanit', sans-serif;
#             font-size: 1.2rem;
#             font-weight: 500;
#             width: 100%;
#             padding: 1rem;
#             border: none;
#             border-radius: 12px;
#             background: linear-gradient(90deg, var(--accent-primary), var(--accent-secondary));
#             color: #ffffff;
#             cursor: pointer;
#             transition: all 0.3s ease;
#             margin-top: 1rem;
#             display: flex;
#             align-items: center;
#             justify-content: center;
#             gap: 0.5rem;
#         }
#         button:hover {
#             transform: translateY(-3px);
#             box-shadow: 0 8px 20px rgba(124, 58, 237, 0.3);
#         }
#         button:disabled {
#             cursor: not-allowed;
#             background: var(--bg-light);
#         }
# 
#         #loader {
#             display: none;
#             margin-top: 1.5rem;
#             text-align: center;
#         }
#         .spinner {
#             border: 4px solid var(--bg-light);
#             border-top: 4px solid var(--accent-primary);
#             border-radius: 50%;
#             width: 40px;
#             height: 40px;
#             animation: spin 1s linear infinite;
#             margin: 0 auto 1rem;
#         }
#         @keyframes spin {
#             0% { transform: rotate(0deg); }
#             100% { transform: rotate(360deg); }
#         }
# 
#         #results-container {
#             display: none;
#             margin-top: 2.5rem;
#             border-top: 1px solid var(--border-color);
#             padding-top: 2rem;
#         }
# 
#         .result-grid {
#             display: grid;
#             grid-template-columns: 1fr 1fr;
#             gap: 1.5rem;
#             align-items: start;
#         }
# 
#         .result-card {
#             background: var(--bg-medium);
#             padding: 1.5rem;
#             border-radius: 16px;
#             border: 1px solid var(--border-color);
#         }
#         .result-card h3 {
#             margin-top: 0;
#             margin-bottom: 1rem;
#             font-size: 1.2rem;
#             border-bottom: 1px solid var(--border-color);
#             padding-bottom: 0.75rem;
#         }
#         #main-result h2 {
#             font-size: 2rem;
#             margin: 0;
#             font-weight: 700;
#             text-align: center;
#         }
#         #main-result p {
#             text-align: center;
#             color: var(--text-secondary);
#             margin-top: 0.25rem;
#         }
#         #main-result .icon {
#             font-size: 3rem;
#             text-align: center;
#             margin-bottom: 1rem;
#         }
#         .OK { color: var(--success-color); }
#         .OTP { color: var(--info-color); }
#         .spam { color: var(--warning-color); }
#         .scam { color: var(--danger-color); }
# 
#         #reason-list {
#             list-style-type: none;
#             padding: 0;
#             margin: 0;
#         }
#         #reason-list li {
#             background-color: var(--bg-light);
#             padding: 0.5rem 1rem;
#             border-radius: 8px;
#             margin-bottom: 0.5rem;
#             font-family: 'Menlo', 'Consolas', monospace;
#             font-size: 0.9rem;
#             display: flex;
#             justify-content: space-between;
#         }
#         #reason-list .weight {
#             color: var(--text-secondary);
#         }
# 
#         @media (max-width: 768px) {
#             .result-grid {
#                 grid-template-columns: 1fr;
#             }
#             .header h1 { font-size: 2.2rem; }
#             body { padding: 1rem 0.5rem; }
#             .container { padding: 1.5rem; }
#         }
# 
#     </style>
# </head>
# <body>
#     <div class="container" data-aos="fade-up">
#         <div class="header">
#             <h1>SMS Analyzer üõ°Ô∏è</h1>
#             <p class="tagline">‡∏£‡∏∞‡∏ö‡∏ö AI ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° Spam ‡πÅ‡∏•‡∏∞ Scam</p>
#         </div>
# 
#         <form id="sms-form">
#             <textarea id="sms-input" name="sms_message" placeholder="‡∏ß‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° SMS ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà..." required></textarea>
#             <button type="submit" id="submit-btn">
#                 <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="m21.73 18-8-14a2 2 0 0 0-3.46 0l-8 14A2 2 0 0 0 4 21h16a2 2 0 0 0 1.73-3Z"></path><path d="M12 9v4"></path><path d="M12 17h.01"></path></svg>
#                 <span>‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°</span>
#             </button>
#         </form>
# 
#         <div id="loader">
#             <div class="spinner"></div>
#             <p>‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå...</p>
#         </div>
# 
#         <div id="results-container" data-aos="fade-up" data-aos-delay="200">
#             <div class="result-grid">
#                 <div class="result-card" id="main-result">
#                     </div>
#                 <div class="result-card">
#                     <h3>üìä ‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô</h3>
#                     <canvas id="probaChart"></canvas>
#                 </div>
#                 <div class="result-card">
#                     <h3>üîë ‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•‡∏´‡∏•‡∏±‡∏Å‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à</h3>
#                     <ul id="reason-list">
#                         </ul>
#                 </div>
#                 <div class="result-card">
#                     <h3>‚öôÔ∏è ‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡∏ó‡∏µ‡πà‡∏™‡∏Å‡∏±‡∏î‡πÑ‡∏î‡πâ</h3>
#                     <pre id="features-pre"></pre>
#                 </div>
#             </div>
#         </div>
#     </div>
# 
#     <script src="https://unpkg.com/aos@2.3.1/dist/aos.js"></script>
#     <script>
#         AOS.init({ duration: 800, once: true });
# 
#         const form = document.getElementById('sms-form');
#         const smsInput = document.getElementById('sms-input');
#         const submitBtn = document.getElementById('submit-btn');
#         const loader = document.getElementById('loader');
#         const resultsContainer = document.getElementById('results-container');
# 
#         const mainResultEl = document.getElementById('main-result');
#         const reasonListEl = document.getElementById('reason-list');
#         const featuresPreEl = document.getElementById('features-pre');
#         const probaChartCanvas = document.getElementById('probaChart');
#         let probaChart = null;
# 
#         form.addEventListener('submit', async (e) => {
#             e.preventDefault();
#             resultsContainer.style.display = 'none';
#             loader.style.display = 'block';
#             submitBtn.disabled = true;
#             submitBtn.querySelector('span').textContent = "‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•...";
# 
#             const formData = new FormData(form);
# 
#             try {
#                 const response = await fetch('/predict', { method: 'POST', body: formData });
#                 const data = await response.json();
# 
#                 if (data.error) {
#                     alert('‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: ' + data.error);
#                 } else {
#                     displayResults(data);
#                 }
#             } catch (error) {
#                 alert('‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏Å‡∏±‡∏ö‡πÄ‡∏ã‡∏¥‡∏£‡πå‡∏ü‡πÄ‡∏ß‡∏≠‡∏£‡πå');
#             } finally {
#                 loader.style.display = 'none';
#                 submitBtn.disabled = false;
#                 submitBtn.querySelector('span').textContent = "‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°";
#             }
#         });
# 
#         function displayResults(data) {
#             const icons = { 'OK': '‚úÖ', 'OTP': 'üîë', 'spam': 'üì¢', 'scam': 'üíÄ' };
#             const messages = {
#                 'OK': '‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡∏µ‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ',
#                 'OTP': '‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡∏µ‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏´‡∏±‡∏™‡∏ú‡πà‡∏≤‡∏ô‡πÉ‡∏ä‡πâ‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß',
#                 'spam': '‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡∏µ‡πâ‡∏°‡∏µ‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏™‡πÅ‡∏õ‡∏°/‡πÇ‡∏Ü‡∏©‡∏ì‡∏≤',
#                 'scam': '‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡∏µ‡πâ‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏°‡∏¥‡∏à‡∏â‡∏≤‡∏ä‡∏µ‡∏û!'
#             };
#             mainResultEl.innerHTML = `
#                 <div class="icon">${icons[data.prediction] || '‚ùì'}</div>
#                 <h2 class="${data.prediction}">${data.prediction}</h2>
#                 <p>${messages[data.prediction] || '‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏£‡∏∞‡∏ö‡∏∏‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡πÑ‡∏î‡πâ'}</p>
#             `;
# 
#             if (probaChart) { probaChart.destroy(); }
#             const chartData = {
#                 labels: data.probabilities.map(p => p.label),
#                 datasets: [{
#                     label: '‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô (%)',
#                     data: data.probabilities.map(p => p.score),
#                     backgroundColor: [
#                         'rgba(34, 197, 94, 0.6)',
#                         'rgba(59, 130, 246, 0.6)',
#                         'rgba(245, 158, 11, 0.6)',
#                         'rgba(239, 68, 68, 0.6)',
#                     ],
#                     borderColor: [
#                         '#22c55e', '#3b82f6', '#f59e0b', '#ef4444'
#                     ],
#                     borderWidth: 1
#                 }]
#             };
#             probaChart = new Chart(probaChartCanvas, {
#                 type: 'bar',
#                 data: chartData,
#                 options: {
#                     indexAxis: 'y',
#                     scales: {
#                         x: { beginAtZero: true, max: 100 }
#                     },
#                     plugins: { legend: { display: false } }
#                 }
#             });
# 
#             let reasonHtml = '';
#             const reasons = data.reason.split(', ').map(r => r.replace(/'/g, ''));
#             if(data.reason && data.reason !== "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç") {
#                 reasons.forEach(r => {
#                     const [word, weight] = r.split(' : ');
#                     reasonHtml += `<li><span>${word}</span><span class="weight">${parseFloat(weight).toFixed(2)}</span></li>`;
#                 });
#             } else {
#                  reasonHtml = `<li>‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ó‡∏µ‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô</li>`;
#             }
#             reasonListEl.innerHTML = reasonHtml;
# 
#             let featuresText = '';
#             for (const [key, value] of Object.entries(data.features)) {
#                 const formattedValue = typeof value === 'number' ? value.toFixed(4) : value;
#                 featuresText += `${key.padEnd(28)}: ${formattedValue}\n`;
#             }
#             featuresPreEl.textContent = featuresText;
# 
#             resultsContainer.style.display = 'block';
#             resultsContainer.scrollIntoView({ behavior: 'smooth', block: 'start' });
#             AOS.refresh();
#         }
#     </script>
# </body>
# </html>

# Commented out IPython magic to ensure Python compatibility.
# %%writefile /content/drive/MyDrive/SMS_Scam_Final_Project/app.py
# import os, csv, joblib, re, string
# from flask import Flask, request, jsonify, render_template
# import pandas as pd
# from pythainlp.tokenize import word_tokenize
# from pythainlp.corpus import thai_stopwords
# import lime
# import lime.lime_text
# 
# # Setup Paths and Flask App
# DRIVE_PROJECT_PATH = '/content/drive/MyDrive/SMS_Scam_Final_Project/'
# TEMPLATE_DIR = os.path.join(DRIVE_PROJECT_PATH, 'templates')
# app = Flask(__name__, template_folder=TEMPLATE_DIR)
# 
# # ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÑ‡∏õ‡πÉ‡∏ä‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡∏ó‡∏µ‡πà‡∏£‡∏ß‡∏°‡∏ó‡∏∏‡∏Å‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÅ‡∏•‡πâ‡∏ß
# pipeline_path = os.path.join(DRIVE_PROJECT_PATH, 'final_sms_classifier_production.joblib')
# encoder_path = os.path.join(DRIVE_PROJECT_PATH, 'label_encoder.joblib')
# 
# 
# # Load Models
# try:
#     pipeline = joblib.load(pipeline_path)
#     le = joblib.load(encoder_path)
#     print("‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÅ‡∏•‡∏∞‡∏ï‡∏±‡∏ß‡πÄ‡∏Ç‡πâ‡∏≤‡∏£‡∏´‡∏±‡∏™‡πÅ‡∏•‡πâ‡∏ß")
# except Exception as e:
#     print(f"Error : {e}")
#     pipeline, le = None, None
# 
# # Helper Function for Text Processing
# def text_process(text: str) -> str:
#     if not isinstance(text, str):
#         return ""
#     emoji_pattern = re.compile(
#         "["
#         u"\\U0001F600-\\U0001F64F" u"\\U0001F300-\\U0001F5FF" u"\\U0001F680-\\U0001F6FF"
#         u"\\U0001F1E0-\\U0001F1FF" u"\\U00002702-\\U000027B0" u"\\U000024C2-\\U0001F251"
#         "]+", flags=re.UNICODE)
#     text = emoji_pattern.sub(r'', text)
#     tokens = word_tokenize(text, engine='newmm')
#     stopwords_list = list(thai_stopwords())
#     punctuations_to_remove = string.punctuation.replace('%', '')
#     processed_tokens = [word for word in tokens if word not in stopwords_list and word not in punctuations_to_remove]
#     return " ".join(processed_tokens)
# 
# # Feature Creation Function
# def create_features(df_input: pd.DataFrame) -> pd.DataFrame:
#     df_out = df_input.copy()
#     df_out['message'] = df_out['message'].astype(str)
#     df_out['message_length'] = df_out['message'].str.len()
#     df_out['digit_count'] = df_out['message'].str.count(r'\\d')
#     df_out['uppercase_ratio'] = df_out['message'].str.count(r'[A-Z]') / (df_out['message_length'] + 1e-6)
#     df_out['repeated_punctuation_count'] = df_out['message'].str.count(r'([!?.])\\1{1,}')
#     df_out['url_count'] = df_out['message'].str.count(r'https?://\\S+|www\\.\\S+|\\S+\\.\\S+')
#     short_url_pattern = 'bit.ly|cutt.ly|t.co|rebrand.ly|shorturl.asia|line.me|lin.ee'
#     df_out['has_short_url'] = df_out['message'].str.contains(short_url_pattern, case=False, regex=True).astype(int)
#     phone_pattern = r'(\\d{2,3}-\\d{3,4}-\\d{4}|\\d{9,10})'
#     df_out['has_phone'] = df_out['message'].str.contains(phone_pattern, regex=True).astype(int)
#     df_out['has_line_keyword'] = df_out['message'].str.contains('Line|‡πÑ‡∏•‡∏ô‡πå|‡πÅ‡∏≠‡∏î‡πÑ‡∏•‡∏ô‡πå', case=False, regex=True).astype(int)
#     advertisement_keywords = ['‡πÇ‡∏õ‡∏£‡πÇ‡∏°‡∏ä‡∏±‡πà‡∏ô', '‡∏™‡πà‡∏ß‡∏ô‡∏•‡∏î', '‡∏ä‡πâ‡∏≠‡∏õ', '‡∏™‡∏≤‡∏Ç‡∏≤', '‡∏Ñ‡∏•‡∏¥‡∏Å‡πÄ‡∏•‡∏¢', '‡πÅ‡∏ö‡∏£‡∏ô‡∏î‡πå', '‡∏Ñ‡∏≠‡∏•‡πÄ‡∏•‡∏Ñ‡∏ä‡∏±‡πà‡∏ô', '‡πÇ‡∏Ñ‡πâ‡∏î']
#     df_out['advertisement_words_count'] = df_out['message'].str.count('|'.join(advertisement_keywords)).astype(int)
#     brand_keywords = ['Lazada', 'Shopee', 'AIS', 'dtac', 'True', 'KFC', 'Grab', 'Central', 'Uniqlo', 'SCB', 'KBank']
#     df_out['is_known_brand'] = df_out['message'].str.contains('|'.join(brand_keywords), case=False, regex=True).astype(int)
#     df_out['is_gambling_keyword'] = df_out['message'].str.contains('‡πÄ‡∏ß‡πá‡∏ö‡∏ï‡∏£‡∏á|PG|JOKER|‡∏™‡∏•‡πá‡∏≠‡∏ï|‡∏ö‡∏≤‡∏Ñ‡∏≤‡∏£‡πà‡∏≤|‡∏Ñ‡∏≤‡∏™‡∏¥‡πÇ‡∏ô|‡πÅ‡∏ó‡∏á‡∏´‡∏ß‡∏¢|‡∏ù‡∏≤‡∏Å‡∏ñ‡∏≠‡∏ô|‡∏¢‡∏π‡∏™‡πÉ‡∏´‡∏°‡πà|‡πÅ‡∏ï‡∏Å‡∏á‡πà‡∏≤‡∏¢', case=False, regex=True).astype(int)
#     df_out['is_loan_keyword'] = df_out['message'].str.contains('‡∏™‡∏¥‡∏ô‡πÄ‡∏ä‡∏∑‡πà‡∏≠|‡πÄ‡∏á‡∏¥‡∏ô‡∏î‡πà‡∏ß‡∏ô|‡∏≠‡∏ô‡∏∏‡∏°‡∏±‡∏ï‡∏¥‡πÑ‡∏ß|‡∏ß‡∏á‡πÄ‡∏á‡∏¥‡∏ô‡∏Å‡∏π‡πâ|‡∏Å‡∏π‡πâ‡πÑ‡∏î‡πâ|‡πÑ‡∏°‡πà‡πÄ‡∏ä‡πá‡∏Ñ‡∏ö‡∏π‡πÇ‡∏£', case=False, regex=True).astype(int)
#     df_out['is_job_offer_keyword'] = df_out['message'].str.contains('‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏ö‡πâ‡∏≤‡∏ô|‡∏£‡∏≤‡∏¢‡πÑ‡∏î‡πâ‡∏ï‡πà‡∏≠‡∏ß‡∏±‡∏ô|part-time|‡∏û‡∏≤‡∏£‡πå‡∏ó‡πÑ‡∏ó‡∏°‡πå|‡∏£‡∏±‡∏ö‡∏™‡∏°‡∏±‡∏Ñ‡∏£', case=False, regex=True).astype(int)
#     emotional_words = ['‡∏ü‡∏£‡∏µ', '‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•', '‡πÅ‡∏à‡∏Å', '‡πÇ‡∏ö‡∏ô‡∏±‡∏™', '‡πÄ‡∏Ñ‡∏£‡∏î‡∏¥‡∏ï', '‡∏£‡∏±‡∏ö‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡πå', '‡πÄ‡∏á‡∏¥‡∏ô‡∏Ñ‡∏∑‡∏ô', '‡∏î‡πà‡∏ß‡∏ô', '‡∏†‡∏≤‡∏¢‡πÉ‡∏ô', '‡∏à‡∏≥‡∏Å‡∏±‡∏î', '‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô', '‡∏ú‡∏¥‡∏î‡∏õ‡∏Å‡∏ï‡∏¥', '‡∏ñ‡∏π‡∏Å‡∏£‡∏∞‡∏á‡∏±‡∏ö', '‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤']
#     df_out['emotional_words_count'] = df_out['message'].str.count('|'.join(emotional_words)).astype(int)
#     obfuscated_pattern = r'\\b[‡∏Å-‡∏Æ]*[a-zA-Z0-9]+[‡∏Å-‡∏Æ]+[a-zA-Z0-9]*\\b'
#     df_out['obfuscated_word_count'] = df_out['message'].str.count(obfuscated_pattern)
#     df_out['processed_message'] = df_out['message'].apply(text_process)
#     df_out['word_diversity_ratio'] = df_out['processed_message'].apply(
#         lambda x: len(set(x.split())) / (len(x.split()) + 1e-6)
#     )
#     return df_out
# 
# # Main Prediction Function
# def analyze_sms(sms_text):
#     if not pipeline or not le:
#         return {"error": "Model not loaded."}
#     try:
#         features_df = create_features(pd.DataFrame([{'message': sms_text}]))
#         prediction_encoded = pipeline.predict(features_df)[0]
#         prediction = le.inverse_transform([prediction_encoded])[0]
#         probabilities = pipeline.predict_proba(features_df)[0]
#         prob_list = [{"label": class_name, "score": round(probabilities[i] * 100, 2)} for i, class_name in enumerate(le.classes_)]
#         prob_list.sort(key=lambda x: x['score'], reverse=True)
# 
#         def lime_predict_fn(texts):
#             lime_df = create_features(pd.DataFrame(texts, columns=['message']))
#             return pipeline.predict_proba(lime_df)
# 
#         explainer = lime.lime_text.LimeTextExplainer(class_names=le.classes_, bow=False, split_expression=word_tokenize)
#         exp = explainer.explain_instance(sms_text, lime_predict_fn, num_features=5, labels=[prediction_encoded])
# 
#         reason_parts = [f"'{word} : {weight:.4f}'" for word, weight in exp.as_list(label=prediction_encoded) if weight > 0]
#         reason_string = ", ".join(reason_parts) if reason_parts else "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç"
# 
#         features_dict = features_df.drop(columns=['message', 'processed_message'], errors='ignore').iloc[0].to_dict()
# 
#         return {
#             "prediction": prediction,
#             "probabilities": prob_list,
#             "reason": reason_string,
#             "features": features_dict
#         }
#     except Exception as e:
#         return {"error": str(e)}
# 
# # Routes
# @app.route('/')
# def home():
#     if not os.path.exists(TEMPLATE_DIR):
#         os.makedirs(TEMPLATE_DIR)
#     html_file_path = os.path.join(TEMPLATE_DIR, 'index.html')
#     if not os.path.exists(html_file_path):
#         with open(html_file_path, 'w') as f:
#             f.write('<!DOCTYPE html><html><head><title>Analyzer</title></head><body><h1>Update index.html!</h1></body></html>')
# 
#     return render_template('index.html')
# 
# @app.route('/predict', methods=['POST'])
# def predict():
#     if 'sms_message' not in request.form:
#         return jsonify({"error": "Missing 'sms_message' in form data."}), 400
#     sms_text = request.form['sms_message']
#     if not sms_text.strip():
#         return jsonify({"error": "SMS message cannot be empty."}), 400
# 
#     result = analyze_sms(sms_text)
#     return jsonify(result)

from pyngrok import ngrok
import sys

PROJECT_DIR = "/content/drive/MyDrive/SMS_Scam_Final_Project"
if PROJECT_DIR not in sys.path:
    sys.path.append(PROJECT_DIR)

from app import app

public_url = ngrok.connect(5000)
print("---" * 10)
print(f"Click here! : {public_url}")
print("---" * 10)

app.run(port=5000)